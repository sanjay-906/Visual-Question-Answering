## Visual-Question-Answering

- VQA involves training ml model which can answer questions based on a given image
- Implemented using Text and Vision transformers

## Dataset
![daquar-dataset](https://github.com/sanjay-906/Visual-Question-Answering/assets/99668976/8c9171a2-5333-48f5-9477-4a3476a1d9ed)

- DAQUAR (DAtaset for QUestion Answering on Real-world images) is a dataset of human question answer pairs about images

## Model Architecture
![arch](https://github.com/sanjay-906/Visual-Question-Answering/assets/99668976/ff494cc8-ca9c-4f5e-80ec-b33b42a67c7a)

- The outputs of both the transformers are merged in a fully connected layer

## Predictions
![predictions](https://github.com/sanjay-906/Visual-Question-Answering/assets/99668976/38121ba3-f94b-4b7f-b0e7-48a63bdea5ce)

## Try Out
OUTPUT: <a href="https://huggingface.co/spaces/sanjay-906/VQA" target="_blank">Click Here</a>

